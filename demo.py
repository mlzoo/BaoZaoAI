from threading import Thread
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer, TextStreamer
from transformers.generation.stopping_criteria import StoppingCriteria
import streamlit as st

# åˆå§‹åŒ–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def load_model():
    model_name = "MLZoo/Qwen2.5-1.5B-Instruct-DPO-bad-boy"
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
    model = AutoModelForCausalLM.from_pretrained(model_name)
    return model, tokenizer

model, tokenizer = load_model()

class StopOnTokens(StoppingCriteria):
    def __init__(self, stop_ids):
        self.stop_ids = stop_ids

    def __call__(self, input_ids, scores, **kwargs):
        # æ£€æŸ¥æœ€åä¸€ä¸ªç”Ÿæˆçš„tokenæ˜¯å¦æ˜¯åœæ­¢token
        for stop_id in self.stop_ids:
            if input_ids[0][-1] == stop_id:
                return True
        return False

def generate_response_streaming(chat_history):
    # æ„å»ºå¯¹è¯å†å²
    messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ²¡ç´ è´¨çš„äººï¼Œè¯·ç”¨æš´èºçš„è¯­æ°”å›å¤æˆ‘ï¼Œå°½é‡å¤šè¯´ä¸€äº›ã€‚"}]
    # æ·»åŠ å†å²å¯¹è¯
    for msg in chat_history:
        messages.append({
            "role": "user" if msg["role"] == "user" else "assistant",
            "content": msg["content"]
        })
    
    input_text = tokenizer.apply_chat_template(messages, tokenize=False)
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    
    # ä½¿ç”¨ streamer è¿›è¡Œç”Ÿæˆ
    streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
    
    # è®¾ç½®ç”Ÿæˆå‚æ•°
    generation_kwargs = {
        "inputs": inputs["input_ids"],
        "max_length": 2048,  # å¢åŠ æœ€å¤§é•¿åº¦ä»¥æ”¯æŒæ›´é•¿çš„å¯¹è¯
        "temperature": 0.7,
        "top_p": 0.9,
        "do_sample": True,
        "streamer": streamer,
        "stopping_criteria": [StopOnTokens([tokenizer.eos_token_id])],
    }
    
    # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿›è¡Œç”Ÿæˆ
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    
    # å®æ—¶è¾“å‡ºç”Ÿæˆçš„æ–‡æœ¬
    token_counts = 0
    
    # åˆ›å»ºä¸€ä¸ªç©ºçš„å ä½ç¬¦
    message_placeholder = st.empty()
    full_response = ""
    
    for new_text in streamer:
        if token_counts < 4:
            token_counts += 1
            continue
        full_response += new_text
        # æ›´æ–°æ˜¾ç¤ºçš„æ–‡æœ¬
        message_placeholder.markdown(full_response + "â–Œ")
    
    # æ˜¾ç¤ºå®Œæ•´çš„å›å¤
    message_placeholder.markdown(full_response)
    return full_response

# Streamlitç•Œé¢è®¾ç½®
st.title("æš´èºAIå“¥ ğŸ¤–")
st.write("æˆ‘æ˜¯DPO trainå‡ºæ¥çš„æš´èºAIå“¥ï¼Œæœ‰ä»€ä¹ˆé—®é¢˜å°½ç®¡é—®æˆ‘ï¼")

# åˆå§‹åŒ–èŠå¤©å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# æ¥æ”¶ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("åœ¨è¿™é‡Œè¾“å…¥ä½ çš„é—®é¢˜..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°èŠå¤©å†å²
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ç”ŸæˆåŠ©æ‰‹å›å¤
    with st.chat_message("assistant"):
        response = generate_response_streaming(st.session_state.messages)
        st.session_state.messages.append({"role": "assistant", "content": response})